---
id: ismar-ai-docent-2025
year: 2025
type:
scope: Demo

thumbnail: "/src/assets/publications/glasses.png"

title:
  en: "Towards Mixed Reality AI Docents: Egocentric Smart Glasses with Vision and LLM Interaction"
  kr: "혼합현실 AI 도슨트를 향하여: 시각 인식과 LLM 상호작용을 활용한 에고센트릭 스마트 글래스"

authors:
  - name: Jongyoon Lim
    role: first
  - name: Jusub Kim
  - name: Sangyong Kim
  - name: Yongsoon Choi

venue:
  en: "IEEE ISMAR 2025 Demo"
  kr: "IEEE ISMAR 2025 데모"

media:
  video: "https://www.youtube.com/embed/uqBGr7pi2UI?si=qNnpOl5HOW0YqD2g"

links:
  pdf: "src/assets/publications/AIglasses.pdf"
  doi: ""

tags:
  - Mixed Reality
  - Smart Glasses
  - AI Docent
  - BLE Localization
  - Image Recognition
  - LLM

body:
  en: |
    We present an AI-driven docent framework as a foundational step toward Mixed Reality (MR) museum interaction. Conventional museum guidance systems, such as QR-based or audio guides, provide limited one-way interaction and require frequent device handling, which can interrupt immersion. In contrast, our system enables egocentric, hands-free, and bidirectional interaction through smart glasses equipped with a camera, microphone, and speaker.

    The proposed system integrates BLE-based localization, camera-based image recognition, and large language model (LLM)-driven conversational interaction. BLE signals are used to estimate proximity to nearby artworks, narrowing the recognition space and improving robustness in dense exhibition environments. Upon user command, images captured by the smart glasses are analyzed in conjunction with spatial context to identify artworks accurately.

    Once an artwork is recognized, users can engage in real-time spoken dialogue with an AI docent. Audio queries are processed through a fine-tuned LLM, generating contextual explanations that are delivered back through the smart glasses’ speaker. This approach minimizes smartphone interaction while supporting personalized, context-aware guidance.

    We conducted a preliminary user study with 16 participants in a museum environment in Seoul, Korea. The results showed high user satisfaction, particularly in recognition accuracy, usefulness of AI explanations, and overall usability. Reported usability ratings averaged 6.5 out of 7 across metrics including goal achievement, clarity of information, and interface intuitiveness. These findings indicate that conversational, location-aware AI agents can significantly enhance museum experiences and provide a strong foundation for future MR systems incorporating AR-based visual overlays and wayfinding.

  kr: |
    본 연구에서는 혼합현실(MR) 기반 박물관 상호작용을 위한 기초 단계로서 AI 도슨트 프레임워크를 제안한다. 기존의 QR 코드나 오디오 가이드 기반 도슨트 시스템은 일방향적인 정보 제공과 잦은 기기 조작을 요구하여 관람 몰입을 저해하는 한계가 있다. 이에 비해 본 시스템은 카메라, 마이크, 스피커를 탑재한 스마트 글래스를 활용하여 에고센트릭하고 핸즈프리 기반의 양방향 상호작용을 가능하게 한다.

    제안된 시스템은 BLE 기반 위치 인식, 카메라 기반 이미지 인식, 그리고 대규모 언어 모델(LLM)을 활용한 대화형 상호작용을 통합한다. BLE 신호를 통해 관람객과 가까운 작품을 추정함으로써 인식 후보를 제한하고, 밀집된 전시 환경에서도 높은 인식 정확도를 확보한다. 사용자의 음성 명령에 따라 스마트 글래스가 이미지를 촬영하면, 공간 정보와 결합된 분석을 통해 작품을 식별한다.

    작품이 인식된 이후, 사용자는 AI 도슨트와 실시간 음성 대화를 통해 작품에 대한 설명을 들을 수 있다. 음성 질의는 LLM을 통해 처리되며, 상황 맥락을 반영한 응답이 스마트 글래스의 스피커를 통해 전달된다. 이 과정은 스마트폰 조작을 최소화하면서도 개인화되고 맥락 인지적인 관람 경험을 제공한다.

    본 시스템의 유효성을 검증하기 위해 서울 소재 박물관에서 16명의 참가자를 대상으로 예비 사용자 실험을 수행하였다. 실험 결과, 이미지 인식의 정확성, AI 설명의 유용성, 전반적인 사용성 측면에서 높은 만족도가 나타났으며, 목표 달성도, 정보 명확성, 인터페이스 직관성 등의 항목에서 평균 7점 만점에 6.5점을 기록하였다. 이러한 결과는 위치 인식과 대화형 AI를 결합한 도슨트 시스템이 박물관 관람 경험을 효과적으로 향상시킬 수 있음을 보여주며, 향후 AR 기반 시각 오버레이와 길 안내 기능을 포함한 MR 도슨트 시스템으로 확장될 가능성을 시사한다.
---